---
title: "ES 292 Ch4 Notes"
author: "Jacob Nesslage"
date: "2024-02-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F, message=F)
```

# Reading: Chapter 4

## Introduction:

Using the geocentric model of the motion of the planets developed by Ptolemy as an example, McElreath makes the point that models can be simultaneously useful and wrong. They can be useful in that they make accurate predictions or approximations of reality (e.g., estimation of the movement of the planets for the geocentric model) while remaining wrong in that they do not capture the underlying process (e.g., the movement of planets is dictated by elliptical orbits, rather than epicycles).

Linear regression - a family of models that attempt to learn about the mean and variance of some measurement using additive combinations of other measurements - is much like the geocentric model. They are descriptive models that can make useful predictions, yet they are often linked to many different process models. This means that we must be careful in their interpretation

This chapter introduces linear regression as a Bayesian procedure. These models employ Guassian (normal) distributions to describe uncertainty about a measurement of interest. 

## 4.1: Why distributions are normal

First, let’s gain some familiarity with the idea of normal distributions through simulations. We will use McElreath’s example:

“Suppose you and a thousand of your closest friends line up on the halfway line of a soccer field (football pitch). Each of you has a coin in your hand. At the sound of the whistle, you begin flipping the coins. Each time a coin comes up heads, that person moves one step towards the left-hand goal. Each time a coin comes up tails, that person moves one step towards the right-hand goal. Each person flips the coin 16 times, follows the implied moves, and then stands still. Now we measure the distance of each person from the halfway line.” (McElreath, pg. 72) 

This example will inform the subsequent simulation below. The following simulations will all result in the characteristic bell curve of a Gaussian or normal distribution, despite the fact that the moves are predicated on coin flips and are described by the binomial distribution. This is because with 16 coin flips, there are many possible locations for people to end up in.

### 4.1.1. Normal by addition

Let’s see the result of the scenario described above, by simulating this experiment in R. 

To show that there’s nothing special about the underlying coin flip, assume instead that each step is different from all the others, a random distance between zero and one yard. Thus a coin is flipped, a distance between zero and one yard is taken in the indicated direction, and the process repeats. 

To simulate this, 
(1) Generate for each person a list of 16 random numbers between −1 and 1 using the runif() command. 
(2) Then we add these steps together to get the position after 16 steps using sum(). 
(3) Replicate this procedure 1000 times using replicate(). 

```{r code 4.1,warn=F}
#If you are unsure what these functions do, run the following:
#?runif()
#?replicate()
require(rethinking)
set.seed(100)
pos <- replicate( 1000 , sum( runif(16,-1,1) ))
dens(pos,norm.comp=T)
```

### 4.1.2. Normal by multiplication

McElreath now turns to another example to illustrate how to achieve a normal distribution through multiplication. 

“Suppose the growth rate of an organism is influenced by a dozen loci, each with several alleles that code for more growth. Suppose also that all of these loci interact with one another, such that each increase growth by a percentage. This means that their effects multiply, rather than add.”

The line below illustrates how to model the multiplicative effects for a single sample

```{r code 4.2}
# We can sample a random growth rate for this example with this line of code:
prod( 1 + runif(12,0,0.1) ) 
```
Now apply this to 10000 samples.

```{r code 4.3}
set.seed(100)
growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) ) 
dens( growth , norm.comp=TRUE )
```
So we achieve a normal distribution, even in the case of multiplication. The caveat here is that this is only true when the multiplying factor is quite small, as these multiplicative effects tend towards the additive approximation for sufficiently small values. 

Let’s compare larger and smaller deviates below to see how magnitude affects the additive approximation:

```{r code 4.4}
set.seed(100)
big <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
dens(big,norm.comp=T)
small <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )
dens(small,norm.comp=T)
```

### 4.1.3. Normal by log-multiplication

Even in the case of large multiplicative effect or deviates, Gaussian distributions can be achieved through log transformation:

```{r code 4.5}
#Before log transformation
set.seed(100)
big <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
dens(big,norm.comp=T)

#After log transformation
log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) ) 
dens(log.big,norm.comp=T)
```

### 4.1.4. Using Gaussian distributions

The justifications for using the Gaussian distribution fall into two broad categories: 

(1) Ontological (related to the study of being)

Gaussian distributions are a widespread pattern, appearing again and again at different scales and in different domains. These distributions are widespread because at their heart, processes often add together fluctuations. Repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread.

(2) Epistemological (related to the study of knowledge)

The Gaussian represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions. That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make

## 4.2: A language for describing models

Here’s the approach to describing models, in abstract. There will be many examples later, but it is important to get the general recipe before seeing these.

(1) First, we recognize a set of variables to work with. Some of these variables are observable. We call these data. Others are unobservable things like rates and averages.
We call these parameters.

(2) We define each variable either in terms of the other variables or in terms of a probability distribution.

(3) The combination of variables and their probability distributions defines a joint generative model that can be used both to simulate hypothetical observations as well
as analyze real ones

McElreath provides the following example:

\begin{equation}
y_{i} ∼ Normal(µ_{i}, σ)
\end{equation}
\begin{equation}
µ_{i} = βx_{i}
\end{equation}
\begin{equation}
β ∼ Normal(0, 10)
\end{equation}
\begin{equation}
σ ∼ Exponential(1)
\end{equation}
\begin{equation}
x_{i} ∼ Normal(0, 1)
\end{equation}

Examples further in the book provide more examples of using this language to describe models.

### 4.2.1. Redescribing the globe tossing model

## 4.3: Gaussian model of height



### 4.3.1. The data

The data contained in data(Howell1) are partial census data for the Dobe
area !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s. For the non-anthropologists reading along, the !Kung San are the most famous foraging
population of the twentieth century, largely because of detailed quantitative studies by people like Howell. 

Load the data and place them into a convenient object with:

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
```

What you have now is a data frame named simply d. I use the name d over and over again
in this book to refer to the data frame we are working with at the moment. I keep its name
short to save you typing. A data frame is a special kind of object in R. It is a table with named columns, corresponding to variables, and numbered rows, corresponding to individual cases. In this example, the cases are individuals. Inspect the structure of the data frame, the same way you can inspect the structure of any symbol in R

```{r}
str( d )

```
We can also use rethinking’s precis summary function, which we’ll also use to summarize
posterior distributions later on:

```{r}
precis( d )
```

This data frame contains four columns. Each column has 544 entries, so there are 544 individuals in these data. Each individual has a recorded height (centimeters), weight (kilograms), age (years), and “maleness” (0 indicating female and 1 indicating male).

We’re going to work with just the height column, for the moment. The column containing the heights is really just a regular old R vector, the kind of list we have been working
with in many of the code examples. You can access this vector by using its name:

```{r,echo=FALSE,results='hide'}
d$height
```

Read the symbol $ as extract, as in extract the column named height from the data frame d.

All we want for now are heights of adults in the sample. The reason to filter out nonadults for now is that height is strongly correlated with age, before adulthood. Later in the chapter, I’ll ask you to tackle the age problem. But for now, better to postpone it. You can filter the data frame down to individuals of age 18 or greater with:

```{r}
d2 <- d[ d$age >= 18 , ]

```

We’ll be working with the data frame d2 now. It should have 352 rows (individuals) in it.

### 4.3.2. The model

Our goal is to model these values using a Gaussian distribution. First, go ahead and plot the distribution of heights, with dens(d2$height). 

```{r}
dens(d2$height)
```

These data look rather Gaussian in shape, as is typical of height data. This may be because height is a sum of many small growth factors. As you saw at the start of the chapter, a distribution of sums tends to converge to a Gaussian distribution. Whatever the reason, adult heights from a single population are nearly always approximately normal.

Let's write the general model for these heights:

\begin{equation}
h_{i} ∼ Normal(µ, σ)
\end{equation}

To complete the model, we’re going to need some priors. The parameters to be estimated
are both µ and σ, so we need a prior Pr(µ, σ), the joint prior probability for all parameters. In most cases, priors are specified independently for each parameter, which amounts to assuming Pr(µ, σ) = Pr(µ) Pr(σ). Then we can write:

\begin{equation} h_i ∼ Normal(µ, σ) \end{equation}
\begin{equation}µ ∼ Normal(178, 20) \end{equation}
\begin{equation}σ ∼ Uniform(0, 50) \end{equation}

where hi is our likelihood, µ is our mean prior, and σ is our standard deviation prior.

The prior for µ is a broad Gaussian prior, centered on 178 cm, with 95% of probability between 178 ± 40 cm. Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218 cm encompasses a huge range of plausible mean heights for human populations. So domain-specific information has gone into this prior. Everyone knows something about human height and can set a reasonable and vague prior of this kind. But in many regression problems, as you’ll see later, using prior information is more subtle, because parameters don’t always have such clear physical meaning.

Whatever the prior, it’s a very good idea to plot your priors, so you have a sense of the
assumption they build into the model. In this case:

```{r}
curve( dnorm( x , 178 , 20 ) , from=100 , to=250 )
```

The σ prior is a truly flat prior, a uniform one, that functions just to constrain σ to have positive probability between zero and 50 cm. View it with:

```{r}
curve( dunif( x , 0 , 50 ) , from=-10 , to=60 )
```
A standard deviation like σ must be positive, so bounding it at zero makes sense. How should we pick the upper bound? In this case, a standard deviation of 50 cm would imply that 95% of individual heights lie within 100 cm of the average height. That’s a very large range.

The prior predictive simulation is an essential part of your modeling.
Once you’ve chosen priors for h, µ, and σ, these imply a joint prior distribution of individual heights. By simulating from this distribution, you can see what your choices imply about observable height. This helps you diagnose bad choices. Lots of conventional choices are indeed bad ones, and we’ll be able to see this through prior predictive simulations.

Okay, so how to do this? You can quickly simulate heights by sampling from the prior,
like you sampled from the posterior back in Chapter 3. Remember, every posterior is also
potentially a prior for a subsequent analysis, so you can process priors just like posteriors.

```{r}
#sample mean as normal distribution centered at 178cm with 20cm SD
sample_mu <- rnorm( 1e4 , 178 , 20 )
#Sample SD as uniform distribution between 0 cm and 50 cm
sample_sigma <- runif( 1e4 , 0 , 50 )
#Normal distribution of prior
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h )

```

Prior predictive simulation is very useful for assigning sensible priors, because it can be quite hard to anticipate how priors influence the observable variables. As an example, consider a much flatter and less informative prior for µ, like µ ∼ Normal(178, 100). Priors with such large standard deviations are quite common in Bayesian models, but they are hardly ever sensible. 

Let’s use simulation again to see the implied heights:

```{r}
sample_mu <- rnorm( 1e4 , 178 , 100 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h )
```

Now the model, before seeing the data, expects 4% of people, those left of the dashed line, to have negative height. It also expects some giants. One of the tallest people in recorded history, Robert Pershing Wadlow (1918–1940) stood 272 cm tall. In our prior predictive simulation, 18% of people (right of solid line) are taller than this

Using scientific knowledge to build priors is not cheating. The important thing is that your prior not be based on the values in the data, but only on what you know about the data before you see it.

### 4.3.3. Grid approximation of the posterior distribution

Since this is the first Gaussian model in the book, and indeed the first model with more than one parameter, it’s worth quickly mapping out the posterior distribution through brute force calculations. This isn’t an encouraged approach, because it is laborious and computationally expensive. 

A little later in this chapter, you’ll use quadratic approximation to estimate the posterior distribution, and that’s the approach you’ll use for several chapters more. Once you have the samples you’ll produce in this subsection, you can compare them to the quadratic approximation in the next.

Unfortunately, doing the calculations here requires some technical tricks that add little,
if any, conceptual insight. So I’m going to present the code here without explanation. You can execute it and keep going for now, but later return and follow the endnote for an explanation of the algorithm.

```{r}
mu.list <- seq( from=150, to=160 , length.out=100 )
sigma.list <- seq( from=7 , to=9 , length.out=100 )
post <- expand.grid( mu=mu.list , sigma=sigma.list )
post$LL <- sapply( 1:nrow(post) , function(i) sum(
  dnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) )
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
  dunif( post$sigma , 0 , 50 , TRUE )
post$prob <- exp( post$prod - max(post$prod) )

```

You can inspect this posterior distribution, now residing in post$prob, using a variety of
plotting commands. You can get a simple contour plot with:

```{r}
contour_xyz( post$mu , post$sigma , post$prob )

```

Or you can plot a simple heat map with:

```{r}
image_xyz( post$mu , post$sigma , post$prob )
```

The functions contour_xyz and image_xyz are both in the rethinking package

### 4.3.4. Sampling from the posterior

To study this posterior distribution in more detail, again I’ll push the flexible approach of sampling parameter values from it. This works just like it did in Chapter 3, when you sampled values of p from the posterior distribution for
the globe tossing example. 

The only new trick is that since there are two parameters, and we want to sample combinations of them, we first randomly sample row numbers in post in proportion to the values in post$prob. Then we pull out the parameter values on those randomly sampled rows. 
This code will do it:

```{r}
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE ,
prob=post$prob )
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
```

You end up with 10,000 samples, with replacement, from the posterior for the height data.

Now that you have these samples, you can describe the distribution of confidence in each
combination of µ and σ by summarizing the samples. Think of them like data and describe
them, just like in Chapter 3. 

For example, to characterize the shapes of the marginal posterior densities of µ and σ, all we need to do is:

```{r}
dens( sample.mu , norm.comp=TRUE)
dens( sample.sigma , norm.comp=TRUE)
```

These densities are very close to being normal distributions.As sample size increases, posterior densities approach the normal distribution. 

If you look closely, though, you’ll notice that the density for σ has a longer
right-hand tail. I’ll exaggerate this tendency a bit later, to show you that this condition is very common for standard deviation parameters.

To summarize the widths of these densities with posterior compatibility intervals:

```{r}
PI( sample.mu )
PI( sample.sigma )
```

Since these samples are just vectors of numbers, you can compute any statistic from them
that you could from ordinary data: mean, median, or quantile, for example.

For example, if the variance is estimated to be near zero, then you know for sure that it can’t be much smaller. But it could be a lot bigger.

Let’s quickly analyze only 20 of the heights from the height data to reveal this issue. To sample 20 random heights from the original list:

```{r}
d3 <- sample( d2$height , size=20 )
```

Now I’ll repeat all the code from the previous subsection, modified to focus on the 20 heights in d3 rather than the original data. I’ll compress all of the code together here:

```{r}
mu.list <- seq( from=150, to=170 , length.out=200 )
sigma.list <- seq( from=4 , to=20 , length.out=200 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list )
post2$LL <- sapply( 1:nrow(post2) , function(i)
  sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,
  log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
  dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,
  prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ]
sample2.sigma <- post2$sigma[ sample2.rows ]
plot( sample2.mu , sample2.sigma , cex=0.5 ,
  col=col.alpha(rangi2,0.1) ,
  xlab="mu" , ylab="sigma" , pch=16 )
```

After executing the code above, you’ll see another scatter plot of the samples from the posterior density, but this time you’ll notice a distinctly longer tail at the top of the cloud of points. You should also inspect the marginal posterior density for σ, averaging over µ, produced with:

```{r}
dens( sample2.sigma , norm.comp=TRUE )
```
This code will also show a normal approximation with the same mean and variance. Now you can see that the posterior for σ is not Gaussian, but rather has a long tail towards higher values.

### 4.3.5. Finding the posterior distribution with quap

Our interest in quadratic approximation, recall, is as a handy way to quickly make
inferences about the shape of the posterior. The posterior’s peak will lie at the maximum a posteriori estimate (MAP), and we can get a useful image of the posterior’s shape by using the quadratic approximation of the posterior distribution at this peak.

To build the quadratic approximation, we’ll use quap, a command in the rethinking
package. The quap function works by using the model definition you were introduced to earlier in this chapter. Each line in the definition has a corresponding definition in the form of R code. 

The engine inside quap then uses these definitions to define the posterior probability
at each combination of parameter values. Then it can climb the posterior distribution and
find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution. Remember: This procedure is very similar to what many non-Bayesian procedures do, just without any priors.

Let’s begin by repeating the code to load the data and select out the adults:

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

```

Now we’re ready to define the model, using R’s formula syntax. The model definition in this case is just as before, but now we’ll repeat it with each corresponding line of R code shown below:

\begin{equation} hi ∼ Normal(µ, σ) \end{equation} 

is written in R as height ~ dnorm(mu,sigma)

\begin{equation} µ ∼ Normal(178, 20) \end{equation}
is written in R as mu ~ dnorm(178,20)

\begin{equation} σ ∼ Uniform(0, 50) \end{equation} 
is written in R as sigma ~ dunif(0,50)

Now place the R code equivalents into an alist. Here’s an alist of the formulas above:

```{r}
flist <- alist(
  height ~ dnorm( mu , sigma ) ,
  mu ~ dnorm( 178 , 20 ) ,
  sigma ~ dunif( 0 , 50 )
)

```

Note the commas at the end of each line, except the last. These commas separate each line
of the model definition.

Fit the model to the data in the data frame d2 with:

```{r}
m4.1 <- quap( flist , data=d2 )
```

After executing this code, you’ll have a fit model stored in the symbol m4.1. Now take a look at the posterior distribution:

```{r}
 precis( m4.1 )

```

These numbers provide Gaussian approximations for each parameter’s marginal distribution.
This means the plausibility of each value of µ, after averaging over the plausibilities of each value of σ, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4.

The priors we used before are very weak, both because they are nearly flat and because
there is so much data. So I’ll splice in a more informative prior for µ, so you can see the effect. All I’m going to do is change the standard deviation of the prior to 0.1, so it’s a very narrow prior. I’ll also build the formula right into the call to quap this time.


```{r}
m4.2 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu ~ dnorm( 178 , 0.1 ) ,
    sigma ~ dunif( 0 , 50 )
  ) , data=d2 )
precis( m4.2 )

```

Notice that the estimate for µ has hardly moved off the prior. The prior was very concentrated around 178. So this is not surprising. But also notice that the estimate for σ has changed quite a lot, even though we didn’t change its prior at all. Once the golem is certain that the mean is near 178—as the prior insists—then the golem has to estimate σ conditional on that fact.

This results in a different posterior for σ, even though all we changed is prior information about the other parameter.

### 4.3.6. Sampling from a quap

The above explains how to get a quadratic approximation of the posterior, using quap. But how do you then get samples from the quadratic approximate posterior distribution? 

The answer is rather simple, but non-obvious, and it requires recognizing that a quadratic approximation to a posterior distribution with more than one parameter dimension—µ and σ each contribute one dimension—is just a multi-dimensional Gaussian distribution.

As a consequence, when R constructs a quadratic approximation, it calculates not only
standard deviations for all parameters, but also the covariances among all pairs of parameters. Just like a mean and standard deviation (or its square, a variance) are sufficient to describe a one-dimensional Gaussian distribution, a list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see this matrix of variances and covariances, for model m4.1, use:

```{r}
vcov( m4.1 )
```

The above is a variance-covariance matrix. It is the multi-dimensional glue of a quadratic approximation, because it tells us how each parameter relates to every other parameter in the posterior distribution. A variance-covariance matrix can be factored into two
elements: 
(1) a vector of variances for the parameters and 
(2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others. 

This decomposition is usually easier to understand. So let’s do that now:

```{r}
diag( vcov( m4.1 ) )
cov2cor( vcov( m4.1 ) )
```

The two-element vector in the output is the list of variances. If you take the square root of this vector, you get the standard deviations that are shown in precis output. 

The two-by-two matrix in the output is the correlation matrix. Each entry shows the correlation, bounded between −1 and +1, for each pair of parameters. The 1’s indicate a parameter’s correlation with itself. If these values were anything except 1, we would be worried. The other entries are typically closer to zero, and they are very close to zero in this example. This indicates that learning µ tells us nothing about σ and likewise that learning σ tells us nothing about µ. This is typical of simple Gaussian models of this kind. But it is quite rare more generally, as you’ll see in later chapters.

Okay, so how do we get samples from this multi-dimensional posterior? Now instead
of sampling single values from a simple Gaussian distribution, we sample vectors of values
from a multi-dimensional Gaussian distribution. The rethinking package provides a convenience function to do exactly that:

```{r}
library(rethinking)
post <- extract.samples( m4.1 , n=1e4 )
head(post)

```

You end up with a data frame, post, with 10,000 (1e4) rows and two columns, one column
for µ and one for σ. Each value is a sample from the posterior, so the mean and standard
deviation of each column will be very close to the MAP values from before. You can confirm
this by summarizing the samples:

```{r}
precis(post)

```


## 4.4: Linear Prediction

What we’ve done above is a Gaussian model of height in a population of adults. But it
doesn’t really have the usual feel of “regression” to it. Typically, we are interested in modeling how an outcome is related to some other variable, a predictor variable. If the predictor variable has any statistical association with the outcome variable, then we can use it to predict the outcome. When the predictor variable is built inside the model in a particular way, we’ll have linear regression.

So now let’s look at how height in these Kalahari foragers (the outcome variable) covaries
with weight (the predictor variable). This isn’t the most thrilling scientific question, I know.But it is an easy relationship to start with, and if it seems dull, it’s because you don’t have a theory about growth and life history in mind. If you did, it would be thrilling. We’ll try later on to add some of that thrill, when we reconsider this example from a more causal perspective. Right now, I ask only that you focus on the mechanics of estimating an association between two variables.

Go ahead and plot adult height and weight against one another:

```{r}
 library(rethinking)
data(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ]
plot( d2$height ~ d2$weight )

```

The plot suggests that knowing a person’s weight helps you predict height.

To make this vague observation into a more precise quantitative model that relates values
of weight to plausible values of height, we need some more technology. How do we take
our Gaussian model from the previous section and incorporate predictor variables?


### 4.4.1. The linear model strategy

The strategy is to make the parameter for the mean of a Gaussian distribution, µ, into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the linear model. The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship.

With a linear model, some of the parameters now stand for the strength of association between the mean of the outcome, µ, and the value of some other variable.

Here’s how it works, in the simplest case of only one predictor variable. We’ll wait until
the next chapter to confront more than one predictor. Recall the basic Gaussian model:

\begin{equation} h_i ∼ Normal(µ, σ) \end{equation} 
\begin{equation} µ ∼ Normal(178, 20) \end{equation} 
\begin{equation} σ ∼ Uniform(0, 50) \end{equation}

Now how do we get weight into a Gaussian model of height? Let x be the name for the
column of weight measurements, d2$weight. Let the average of the x values be ¯x, “ex bar”.
Now we have a predictor variable x, which is a list of measures of the same length as h. To get weight into the model, we define the mean µ as a function of the values in x. This is what it looks like, with explanation to follow:

\begin{equation}hi ∼ Normal(µi, σ) \end{equation}
\begin{equation}µi = α + β(xi − ¯x) \end{equation}
\begin{equation}α ∼ Normal(178, 20) \end{equation}
\begin{equation}β ∼ Normal(0, 10) \end{equation}
\begin{equation}σ ∼ Uniform(0, 50) \end{equation}

##### Probability of the data (likelihood)

Let’s begin with just the probability of the observed height, the first line of the model. This is nearly identical to before, except now there is a little index i on the µ as well as the h. You can read hi as “each h” and µi as “each µ.” The mean µ now depends upon unique values on each row i. So the little i on µi indicates that the mean depends upon the row.

##### Linear model

The mean µ is no longer a parameter to be estimated. Rather, as seen in the second line of the model, µi is constructed from other parameters, α and β, and the observed variable x. This line is not a stochastic relationship—there is no ∼ in it, but rather an = in it—because the definition of µi is deterministic. That is to say that, once we know α and β and xi, we know µi with certainty.

The value xi is just the weight value on row i. It refers to the same individual as the
height value, hi, on the same row. 

The parameters α and β are more mysterious. Where did they come from? We made them up. The parameters µ and σ are necessary and sufficient to describe a Gaussian distribution. But α and β are instead devices we invent for manipulating µ, allowing it to vary systematically across cases in the data.Taking the second line:

\begin{equation}µi = α + β(xi − ¯x) \end{equation}

What this tells the regression golem is that you are asking two questions about the mean of the outcome.

(1) What is the expected height when xi = ¯x? The parameter α answers this question,
because when xi = ¯x, µi = α. For this reason, α is often called the intercept. But we
should think not in terms of some abstract line, but rather in terms of the meaning
with respect to the observable variables.

(2) What is the change in expected height, when xi changes by 1 unit? The parameter
β answers this question. It is often called a “slope,” again because of the abstract line. Better to think of it as a rate of change in expectation. 

Jointly these two parameters ask the golem to find a line that relates x to h, a line that passes through α when xi = ¯x and has slope β. That is a task that golems are very good at. It’s up to you, though, to be sure it’s a good question.

##### Priors

The remaining lines in the model define distributions for the unobserved
variables. These variables are commonly known as parameters, and their distributions as priors. There are three parameters: α, β, and σ. You’ve seen priors for α and σ before, although α was called µ back then.

The prior for β deserves explanation. Why have a Gaussian prior with mean zero? This
prior places just as much probability below zero as it does above zero, and when β = 0, weight has no relationship to height. To figure out what this prior implies, we have to simulate the prior predictive distribution.

The goal is to simulate heights from the model, using only the priors. First, let’s consider a range of weight values to simulate over. The range of observed weights will do fine. Then we need to simulate a bunch of lines, the lines implied by the priors for α and β. Here’s how to do it, setting a seed so you can reproduce it exactly:

```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 )

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
col=col.alpha("black",0.2) )

```

For reference, I’ve added a dashed line at zero—no one is shorter than zero—and the “Wadlow” line at 272 cm for the world’s tallest person. The pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen the data, this is a bad model. Can we do better?

We can do better immediately. We know that average height increases with average
weight, at least up to a point. Let’s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead. If you aren’t accustomed to playing with logarithms, that’s okay. There’s more detail in the box at the end of this section.

Defining β as Log-Normal(0,1) means to claim that the logarithm of β has a Normal(0,1)
distribution. Plainly:

R provides the dlnorm and rlnorm densities for working with log-normal distributions. You
can simulate this relationship to see what this means for β

```{r}
b <- rlnorm( 1e4 , 0 , 1 )
dens( b , xlim=c(0,5) , adj=0.1 )
```

If the logarithm of β is normal, then β itself is strictly positive. The reason is that exp(x) is greater than zero for any real number x. This is the reason that Log-Normal priors are commonplace. They are an easy way to enforce positive relationships. So what does this earn us? Do the prior predictive simulation again, now with the Log-Normal prior:

```{r}
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,
xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
from=min(d2$weight) , to=max(d2$weight) , add=TRUE ,
col=col.alpha("black",0.2) )
```

Plotting as before produces the right-hand plot in Figure 4.5. This is much more sensible.
There is still a rare impossible relationship. But nearly all lines in the joint prior for α and β are now within human reason.

We’re fussing about this prior, even though as you’ll see in the next section there is so
much data in this example that the priors end up not mattering. We fuss for two reasons.
First, there are many analyses in which no amount of data makes the prior irrelevant. In such cases, non-Bayesian procedures are no better off. They also depend upon structural features of the model. Paying careful attention to those features is essential. Second, thinking about the priors helps us develop better models.

### 4.4.2.  Finding the posterior distribution

The code needed to approximate the posterior is a straightforward modification of the kind of code you’ve already seen. All we have to do is incorporate our new model for the mean into the model specification inside quap and be sure to add a prior for the new parameter, β. Let’s repeat the model definition, now with the corresponding R code below the mathematical formulation:

\begin{equation}hi ∼ Normal(µi, σ) \end{equation}   
\begin{equation}µi = α + β(xi − ¯x) \end{equation}  
\begin{equation}α ∼ Normal(178, 20) \end{equation}
\begin{equation}β ∼ Log-Normal(0, 1) \end{equation}
\begin{equation}σ ∼ Uniform(0, 50) \end{equation}


height ~ dnorm(mu,sigma)
mu <- a + b*(weight-xbar)
a ~ dnorm(178,20)
b ~ dlnorm(0,1)
sigma ~ dunif(0,50)

The above allows us to build the posterior approximation:

```{r}
# load data again, since it's a long way back
library(rethinking)
data(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ]
# define the average weight, x-bar
xbar <- mean(d2$weight)

m4.3 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*( weight - xbar ) ,
    a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
  ) , data=d2 )

```

### 4.4.3. Interpreting the posterior distribution

One trouble with statistical models is that they are hard to understand. Once you’ve fit the model, it can only report posterior distribution. This is the right answer to the question you asked. But it’s your responsibility to process the answer and make sense of it.

There are two broad categories of processing: 
(1) reading tables and 
(2) plotting simulations. 

For some simple questions, it’s possible to learn a lot just from tables of marginal values. But most models are very hard to understand from tables of numbers alone. A major
difficulty with tables alone is their apparent simplicity compared to the complexity of the model and data that generated them. Once you have more than a couple of parameters in a model, it is very hard to figure out from numbers alone how all of them act to influence prediction.

Plotting the implications of your models instead will allow you to inquire about things that are hard to read from tables:

(1) Whether or not the model fitting procedure worked correctly
(2) The absolute magnitude, rather than merely relative magnitude, of a relationship
between outcome and predictor
(3) The uncertainty surrounding an average relationship
(4) The uncertainty surrounding the implied predictions of the model, as these are
distinct from mere parameter uncertainty

In addition, once you get the hang of processing posterior distributions into plots, you can ask any question you can think of, for any model type. And readers of your results will appreciate a figure much more than they will a table of estimates.

##### Tables of marginal distributions

With the new linear regression trained on the Kalahari data, we inspect the marginal posterior distributions of the parameters:

```{r}
precis( m4.3 )
```

The first row gives the quadratic approximation for α, the second the approximation for β,
and the third approximation for σ. Let’s try to make some sense of them.

Let’s focus on b (β), because it’s the new parameter. Since β is a slope, the value 0.90
can be read as a person 1 kg heavier is expected to be 0.90 cm taller. 89% of the posterior probability lies between 0.84 and 0.97. That suggests that β values close to zero or greatly above one are highly incompatible with these data and this model. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones.

Remember, the numbers in the default precis output aren’t sufficient to describe the
quadratic posterior completely. For that, we also require the variance-covariance matrix.
You can see the covariances among the parameters with vcov.

```{r}
round( vcov( m4.3 ) , 3 )

```

Very little covariation among the parameters in this case. Using pairs(m4.3) shows both
the marginal posteriors and the covariance. In the practice problems at the end of the chapter, you’ll see that the lack of covariance among the parameters results from centering.

##### Plotting posterior inference against the data

It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions. When the model’s predictions don’t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified. But even if you only treat plots as a way to help in interpreting the posterior, they are invaluable.

For simple models like this one, it is possible (but not always easy) to just read the table of numbers and understand what the model says. But for even slightly more complex models, especially those that include interaction effects (Chapter 8), interpreting posterior distributions is hard. Combine with this the problem of incorporating the information in vcov into your interpretations, and the plots are irreplaceable.

We’re going to start with a simple version of that task, superimposing just the posterior
mean values over the height and weight data. Then we’ll slowly add more and more information to the prediction plots, until we’ve used the entire posterior distribution.
We’ll start with just the raw data and a single line. 

The code below plots the raw data, computes the posterior mean values for a and b, then draws the implied line:


```{r}
plot( height ~ weight , data=d2 , col=rangi2 )
post <- extract.samples( m4.3 )
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map*(x - xbar) , add=TRUE )
```

Each point in this plot is a single individual.
The black line is defined by the mean slope β and mean intercept α. This is not a bad line. It certainly looks highly plausible. But there are an infinite number of other highly plausible lines near it. Let’s draw those too.

##### Adding uncertainty around the mean

The posterior mean line is just the posterior mean, the most plausible line in the infinite universe of lines the posterior distribution
has considered. Plots of the average line, like Figure 4.6, are useful for getting an impression of the magnitude of the estimated influence of a variable. But they do a poor job of communicating uncertainty.

So how can we get that uncertainty onto the plot? Together, a combination of α and
β define a line. And so we could sample a bunch of lines from the posterior distribution.
Then we could display those lines on the plot, to visualize the uncertainty in the regression relationship.

To better appreciate how the posterior distribution contains lines, we work with all of
the samples from the model. Let’s take a closer look at the samples now:

```{r}
post <- extract.samples( m4.3 )
post[1:5,]

```

Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by vcov(m4.3). The paired values of a and b on each row define a line. The average of very many of these lines is the posterior mean line. But the scatter around that average is meaningful, because it alters our confidence in the relationship between the predictor and the outcome.

So now let’s display a bunch of these lines, so you can see the scatter. This lesson will be easier to appreciate, if we use only some of the data to begin. Then you can see how adding in more data changes the scatter of the lines. So we’ll begin with just the first 10 cases in d2.

The following code extracts the first 10 cases and re-estimates the model:


```{r}
N <- 10
dN <- d2[ 1:N , ]
mN <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - mean(weight) ) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=dN )
```

Now let’s plot 20 of these lines, to see what the uncertainty looks like.

```{r}
# extract 20 samples from the posterior
post <- extract.samples( mN , n=20 )
# display raw data and sample size
plot( dN$weight , dN$height ,
xlim=range(d2$weight) , ylim=range(d2$height) ,
col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))
# plot the lines, with transparency
for ( i in 1:20 )
curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,
col=col.alpha("black",0.3) , add=TRUE )
```

The last line loops over all 20 lines, using curve to display each.

The result is shown in the upper-left plot in Figure 4.7. By plotting multiple regression
lines, sampled from the posterior, it is easy to see both the highly confident aspects of the relationship and the less confident aspects. The cloud of regression lines displays greater uncertainty at extreme values for weight.

The other plots in Figure 4.7 show the same relationships, but for increasing amounts
of data. Just re-use the code from before, but change N <- 10 to some other value. Notice
that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean.

##### Plotting regression intervals and contours. 

The cloud of regression lines in Figure 4.7 is an appealing display, because it communicates uncertainty about the relationship in a way that many people find intuitive. But it’s more common, and often much clearer, to see the uncertainty displayed by plotting an interval or contour around the average regression line. 

In this section, I’ll walk you through how to compute any arbitrary interval you like, using the underlying cloud of regression lines embodied in the posterior distribution.

Focus for the moment on a single weight value, say 50 kilograms. You can quickly make
a list of 10,000 values of µ for an individual who weighs 50 kilograms, by using your samples from the posterior:

```{r}
post <- extract.samples( m4.3 )
mu_at_50 <- post$a + post$b * ( 50 - xbar )
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )

```

Since the posterior for µ is a distribution, you can find intervals for it, just like for any posterior distribution. To find the 89% compatibility interval of µ at 50 kg, just use the PI command as usual:

```{r}
PI( mu_at_50 , prob=0.89 )
```

What these numbers mean is that the central 89% of the ways for the model to produce the
data place the average height between about 159 cm and 160 cm (conditional on the model
and data), assuming the weight is 50 kg.

That’s good so far, but we need to repeat the above calculation for every weight value
on the horizontal axis, not just when it is 50 kg.

This is made simple by strategic use of the link function, a part of the rethinking
package. What link will do is take your quap approximation, sample from the posterior
distribution, and then compute µ for each case in the data and sample from the posterior
distribution. Here’s what it looks like for the data you used to fit the model:

```{r}
mu <- link( m4.3 )
str(mu)

```

Now what can we do with this big matrix? Lots of things. The function link provides
a posterior distribution of µ for each case we feed it. So above we have a distribution of
µ for each individual in the original data. We actually want something slightly different: a distribution of µ for each unique weight value on the horizontal axis. It’s only slightly harder to compute that, by just passing link some new data:


```{r}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq( from=25 , to=70 , by=1 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link( m4.3 , data=data.frame(weight=weight.seq) )
str(mu)
```
And now there are only 46 columns in mu, because we fed it 46 different values for weight.
To visualize what you’ve got here, let’s plot the distribution of µ values at each height

```{r}
# use type="n" to hide raw data
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value
for ( i in 1:100 )
points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )
```

The final step is to summarize the distribution for each weight value. We’ll use apply,
which applies a function of your choice to a matrix.

```{r}
# summarize the distribution of mu
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

```

Read apply(mu,2,mean) as compute the mean of each column (dimension “2”) of the matrix
mu. Nowmu.mean contains the average µ at each weight value, and mu.PI contains 89% lower
and upper bounds for each weight value. Be sure to take a look inside mu.mean and mu.PI,
to demystify them. They are just different kinds of summaries of the distributions in mu, with each column being for a different weight value. These summaries are only summaries. The “estimate” is the entire distribution.

You can plot these summaries on top of the data with a few lines of R code:

```{r}
# plot raw data
# fading out points to make line and interval more visible
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight
lines( weight.seq , mu.mean )
# plot a shaded region for 89% PI
shade( mu.PI , weight.seq )
```

To summarize, here’s the recipe for generating predictions and intervals from the posterior of a fit model:

(1) Use link to generate distributions of posterior values for µ. The default behavior
of link is to use the original data, so you have to pass it a list of new horizontal axis
values you want to plot posterior predictions across.
(2) Use summary functions like mean or PI to find averages and lower and upper
bounds of µ for each value of the predictor variable.
(3) Finally, use plotting functions like lines and shade to draw the lines and intervals.
Or you might plot the distributions of the predictions, or do further numerical
calculations with them. It’s really up to you.

##### Prediction Intervals

Now let’s walk through generating an 89% prediction interval for actual heights, not just the average height, µ. This means we’ll incorporate the standard deviation σ and its uncertainty as well. Remember, the first line of the statistical model here is:

What you’ve done so far is just use samples from the posterior to visualize the uncertainty in µi, the linear model of the mean. But actual predictions of heights depend also upon the distribution in the first line. The Gaussian distribution on the first line tells us that the model expects observed heights to be distributed around µ, not right on top of it. And the spread around µ is governed by σ. All of this suggests we need to incorporate σ in the predictions somehow.

Here’s how you do it. Imagine simulating heights. For any unique weight value, you sample from a Gaussian distribution with the correct mean µ for that weight, using the correct
value of σ sampled from the same posterior distribution. If you do this for every sample
from the posterior, for every weight value of interest, you end up with a collection of simulated heights that embody the uncertainty in the posterior as well as the uncertainty in the Gaussian distribution of heights. There is a tool called sim which does this:

```{r}
sim.height <- sim( m4.3 , data=list(weight=weight.seq) )
str(sim.height)
```

This matrix is much like the earlier one, mu, but it contains simulated heights, not distributions of plausible average height, µ.

We can summarize these simulated heights in the same way we summarized the distributions of µ, by using apply:

```{r}
 height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

```


```{r}
# plot raw data
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
# draw MAP line
lines( weight.seq , mu.mean )

mu.HPDI <- apply(mu,2,HPDI,prob=0.89)
# draw HPDI region for line
#shade( mu.HPDI , weight.seq )
# draw PI region for simulated heights
shade( height.PI , weight.seq )

```


```{r}
sim.height <- sim( m4.3 , data=list(weight=weight.seq) , n=1e4 )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

# plot raw data
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
# draw MAP line
lines( weight.seq , mu.mean )

mu.HPDI <- apply(mu,2,HPDI,prob=0.89)
# draw HPDI region for line
#shade( mu.HPDI , weight.seq )
# draw PI region for simulated heights
shade( height.PI , weight.seq )
```


## 4.5: Curves from Lines

In the next chapter, you’ll see how to use linear models to build regressions with more than one predictor variable. But before then, it helps to see how to model the outcome as a curved function of a predictor. The models so far all assume that a straight line describes the relationship. But there’s nothing special about straight lines, aside from their simplicity.

We’ll consider two commonplace methods that use linear regression to build curves.
The first is polynomial regression. The second is b-splines. Both approaches work by
transforming a single predictor variable into several synthetic variables. But splines have
some clear advantages. Neither approach aims to do more than describe the function that
relates one variable to another. Causal inference, which we’ll consider much more beginning
in the next chapter, wants more.

### 4.5.1. Polynomial regression

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
```

```{r}
d$weight_s <- ( d$weight - mean(d$weight) )/sd(d$weight)
d$weight_s2 <- d$weight_s^2
m4.5 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b1*weight_s + b2*weight_s2 ,
a ~ dnorm( 178 , 20 ) ,
b1 ~ dlnorm( 0 , 1 ) ,
b2 ~ dnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d )
```

```{r}
precis( m4.5 )
```

```{r}
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 )
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 )
mu <- link( m4.5 , data=pred_dat )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.5 , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )
```

```{r}
plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) )
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )
```

```{r}
d$weight_s3 <- d$weight_s^3
m4.6 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b1*weight_s + b2*weight_s2 + b3*weight_s3 ,
a ~ dnorm( 178 , 20 ) ,
b1 ~ dlnorm( 0 , 1 ) ,
b2 ~ dnorm( 0 , 10 ) ,
b3 ~ dnorm( 0 , 10 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d )
```

```{r}
plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) , xaxt="n" )
at <- c(-2,-1,0,1,2)
labels <- at*sd(d$weight) + mean(d$weight)
axis( side=1 , at=at , labels=round(labels,1) )
```




### 4.5.2. Splines

```{r}
library(rethinking)
data(cherry_blossoms)
d <- cherry_blossoms
precis(d)

```

```{r}
d2 <- d[ complete.cases(d$doy) , ] # complete cases on doy
num_knots <- 15
knot_list <- quantile( d2$year , probs=seq(0,1,length.out=num_knots) )

```

```{r}
library(splines)
B <- bs(d2$year,
knots=knot_list[-c(1,num_knots)] ,
degree=3 , intercept=TRUE )

```

```{r}
plot( NULL , xlim=range(d2$year) , ylim=c(0,1) , xlab="year" , ylab="basis" )
for ( i in 1:ncol(B) ) lines( d2$year , B[,i] )
```

```{r}
m4.7 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + B %*% w ,
a ~ dnorm(100,10),
w ~ dnorm(0,10),
sigma ~ dexp(1)
), data=list( D=d2$doy , B=B ) ,
start=list( w=rep( 0 , ncol(B) ) )) 
```

```{r}
post <- extract.samples( m4.7 )
w <- apply( post$w , 2 , mean )
plot( NULL , xlim=range(d2$year) , ylim=c(-6,6) ,
xlab="year" , ylab="basis * weight" )
for ( i in 1:ncol(B) ) lines( d2$year , w[i]*B[,i] )
```

```{r}
mu <- link( m4.7 )
mu_PI <- apply(mu,2,PI,0.97)
plot( d2$year , d2$doy , col=col.alpha(rangi2,0.3) , pch=16 )
shade( mu_PI , d2$year , col=col.alpha("black",0.5) )

```

```{r}
 m4.7alt <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + sapply( 1:827 , function(i) sum( B[i,]*w ) ) ,
a ~ dnorm(100,1),
w ~ dnorm(0,10),
sigma ~ dexp(1)
),
data=list( D=d2$doy , B=B ) ,
start=list( w=rep( 0 , ncol(B) ) ) )

```



